---
title: "DATA624 Project2"
author: "Samuel I Kigamba"
date: "July 10, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r paged.print=TRUE,  include=FALSE}
library(readxl)
library(skimr)
library(naniar)
library(VIM)
library(MASS)
library(forecast)
library(mixtools)
library(caret)
library(parallel)
library(mlbench)
library(rpart.plot)
library(ggplot2)
library(ggfortify)
library(gridExtra)
library(ggcorrplot)
library(corrplot)
library(RColorBrewer)
library(fpp2)
library(fma)
library(kableExtra)
library(e1071)
library(timeDate)
library(tidyverse)
library(dplyr)
library(tidyr)
library(reshape2)
library(tibble)
library(doParallel)

```


## Instructions

### Overview

This is role playing. I am your new boss. I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me. My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.

### Deliverables

Please submit both RPubs links and .rmd files or other readable formats for technical and non-technical reports. Also submit the excel file showing the prediction of your models for pH.

## Introduction

Our team's analysis seeks to build understanding of the ABC Beverage manufacturing process and the related factors that affect the pH of the company's beverage products. Our goal is to build a model that both predicts product hP, given manufacturing steps and identify which steps appear to have the most impact on pH.

We have been provided with historic data for product batches including data on each manufacturing step along with the final measured pH.  We will start by understanding the dataset.  Specifically are the any missing data, outliers or odd feature distributions that might complicate modeling.  We will then do any necessary data cleaning, split our data into training and testing set so we can more accurately determine model performance on out-of-set data samples.  We will preform a number of different machine learning approaches, touching on different broad prediction approaches including: linear regression, multiple regression, penalized regression, non-linear regression, tree-based, and neural network.  Different methodologies can perform better depending on the nature of the data, so it makes sense to try a number of approaches and choose the one that best handles our specific dataset.  We will then choose the model that performs best and use that to predict final pH on a holdout evaluation dataset.

This model could help the company adapt its processes in a changing regulatory environment.

Note - we are doing an observational study so any correlations we identify would need to be followed up with testing to identify causal relationships.


## 1. Data Exploration

### Dataset

The training data set contains 32 categorical, continuous, or discrete features and 2571 rows, with 267 rows reserved for an evaluation set that lacks the target. That target is `PH`, which should be a continuous variable but has 52 distinct values in the training set. As a result, possible predictive models could include regression, classification, or an ensemble of both.

There are two files provided:

-   **StudentData.xlsx** - The data set we use to train our model. It contains `PH`, the feature we seek to predict.
-   **StudentEvaluation.xlsx** - The data set we use to evaluate our model. It lacks `PH`. Our model will have to be scored by an outside group with knowledge of the actual pH values.

*Note: Both Excel files are in simple CSV format.*


```{r load_data}
# Load beverages data set
#df <- read_excel('C:/Users/wb508205/OneDrive - WBG/Documents/QSA/DATA Science CUNY/DATA 624 Predictive Analytics/HW2 & Project 2/StudentDataTOMODEL.xlsx')
#df_eval <- read_excel('C:/Users/wb508205/OneDrive - WBG/Documents/QSA/DATA Science CUNY/DATA 624 Predictive Analytics/HW2 & Project 2/StudentEvaluationTOPREDICT.xlsx')

df = read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv", header = TRUE)
df_eval = read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv", header = TRUE)

# remove the empty PH column from the evaluation data
df_eval <- df_eval %>%
  dplyr::select(-PH)
```

Below is a list of the variables of interest in the data set:

-   `Brand Code`: categorical, values: A, B, C, D
-   `Carb Volume`:
-   `Fill Ounces`:
-   `PC Volume`:
-   `Carb Pressure`:
-   `Carb Temp`:
-   `PSC`:
-   `PSC Fill`:
-   `PSC CO2`:
-   `Mnf Flow`:
-   `Carb Pressure1`:
-   `Fill Pressure`:
-   `Hyd Pressure1`:
-   `Hyd Pressure2`:
-   `Hyd Pressure3`:
-   `Hyd Pressure4`:
-   `Filler Level`:
-   `Filler Speed`:
-   `Temperature`:
-   `Usage cont`:
-   `Carb Flow`:
-   `Density`:
-   `MFR`:
-   `Balling`:
-   `Pressure Vacuum`:
-   `PH`: **the TARGET we will try to predict.**
-   `Bowl Setpoint`:
-   `Pressure Setpoint`:
-   `Air Pressurer`:
-   `Alch Rel`:
-   `Carb Rel`:
-   `Balling Lvl`:

### Summary Stats

We compiled summary statistics on our dataset to better understand the data before modeling.

```{r data_summary}
# Display summary statistics
skim(df)
```


First, across features, there are numerous missing data--coded as NA--that will need to be imputed. Especially note that 4 rows are missing a `PH` value. We will need to drop these rows as they cannot be used for training. Second, the basic histograms suggest that skewness is prevalent across features. Examples include `PSC CO2` and `MFR`. And third, some of the skewed features appear to show near-zero variance, with a large number of 0 or even negative values, e.g. `Hyd Pressure1` and `Hyd Pressure2`. In general, the skewness and imbalance may require imputation.

### Check Target Bias

If our target, `PH` is particularly skewed, it could lead to biased predictions.

```{r}
hist(df$PH)
```

`PH` is normally distributed with possible outliers on the low and high ends. This distribution suggests a pure classification approach could be problematic as the predictions may favor pH values in the mid-range (where there are more data points). Natural boundaries may exist such that classification adds predictive information. However, given the normal shape, a regression or possible ensemble with regression and classification seems more appropriate.  Also, we note that models may have more problems predicting pH values in the extremes.  There are fewer observations at the low an high pH which means less information to help a model tune for these regions.

### Missing Data


Before continuing, let us better understand any patterns of missingness across predictor features.

```{r echo=FALSE}
# Identify missing data by Feature and display percent breakout
missing <- colSums(df %>% sapply(is.na))
missing_pct <- round(missing / nrow(df) * 100, 2)
stack(sort(missing_pct, decreasing = TRUE))
# Various NA plots to inspect data
knitr::kable(miss_var_summary(df), 
             caption = 'Missing Values',
             format="html", 
             table.attr="style='width:50%;'") %>% 
  kableExtra::kable_styling()
gg_miss_var(df)
gg_miss_upset(df)
```

Notice that approximately 8.25 percent of the rows are missing a value for `MFR`. We may need to drop this feature considering that, as missingness increases, so do the potential negative consequences of imputation. Additionally, the categorical feature `Brand Code` is missing approximately 4.67 percent of its values. Since we do not know whether these values represent another brand or are actually missing, we will create a new feature category 'Unknown' consisting of missing values. The rest of the features are only missing small percentages of values, suggesting that KNN imputation should be safe.

### Distributions

Next, we visualize the distributions of each of the predictor features. The visuals will help us select features for modeling, assess relationships between features and with `PH`, and identify outliers as well as transformations that might improve model resolution.

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare data for ggplot
gather_df <- df %>% 
  drop_na() %>%
  dplyr::select(-c(PH, `Brand Code`)) %>%
  gather(key = 'variable', value = 'value')
# Histogram plots of each variable
ggplot(gather_df) + 
  geom_histogram(aes(x=value, y = ..density..), bins=30) + 
  geom_density(aes(x=value), color='blue') +
  facet_wrap(. ~variable, scales='free', ncol=4)
```

The distribution profiles show the prevalence of kurtosis, specifically right skew in variables `Oxygen Filler`, `PSC`, and `Temperature` and left skew in `Filler Speed` and `MFR`. These deviations from a traditional normal distribution can be problematic for linear regression assumptions, and thus we might need to transform the data. Several features are discrete with limited possible values, e.g. `Pressure Setpoint`. Furthermore, we have a number of bimodel features--see `Air Pressurer`, `Balling`, and `Balling Level`.

Bimodal features in a dataset are problematic but interesting, representing areas of potential opportunity and exploration. They suggest the existence of two different groups, or classes, within a given feature. These groups may have separate but overlapping distributions that could provide powerful predictive power in a model.

Were we tackling in-depth feature engineering in this analysis, we could leverage the package, `mixtools` (see R Vignette). This package helps regress *mixed models* where data can be subdivided into subgroups. We could then add new binary features to indicate for each instance, the distribution to which it belongs.

Here is a quick example showing a possible mix within `Air Pressurer`:


```{r}
# Select `Air Pressurer` column and remove any missing data
df_mix <- df %>% 
  dplyr::select(`Air Pressurer`) %>%
  tidyr::drop_na()
# Calculate mixed distributions for indus
air_pressure_mix <- normalmixEM(df_mix$`Air Pressurer`, 
                            lambda = .5, 
                            mu = c(140, 148), 
                            sigma = 1, 
                            maxit=60)
# Simple plot to illustrate possible bimodal mix of groups
plot(air_pressure_mix, 
     whichplots = 2,
     density = TRUE, 
     main2 = "`Air Pressurer` Possible Distributions", 
     xlab2 = "Air Pressurer")
```

Lastly, several features have relatively normal distributions along with high numbers of values at an extreme. We have no information on whether these extreme values are mistakes, data errors, or otherwise inexplicable. As such, we will need to review each associated feature to determine whether to impute the values, leave them as is, or apply feature engineering.

### Boxplots

We also elected to use boxplots to understand the spread of each feature.

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare data for ggplot
gather_df <- df %>% 
  dplyr::select(-c(PH, `Brand Code`)) %>%
  tidyr::drop_na() %>%
  gather(key = 'variable', value = 'value')
# Boxplots for each variable
gather_df %>% ggplot() + 
  geom_boxplot(aes(x=variable, y=value)) + 
  facet_wrap(. ~variable, scales='free', ncol=6)
```

The boxplots reveal outliers, though none of them seem egregious enough to warrant imputing or removal. Outliers should only be imputed or dropped if we have reason to believe they are errant or contain no critical information.

### Variable Plots

Next, we generate scatter plots of each predictor versus the target to get an idea of the relationship between them.

```{r, fig.height = 10, fig.width = 10, echo=FALSE}
df_features <- df %>% 
  dplyr::select(-c(PH, `Brand Code`))
df_features$PH <- df$PH
df_features <- df_features %>%
  drop_na
feature_count <- ncol(df_features) - 1
# Plot scatter plots of each variable versus the target variable
# Note that we are braking these into sets of 8 features ata  time so the 
# resulting plots are more readable.
sets <- 8
batches <- (feature_count) %/% sets
for (i in 0:batches) {
  start <- i * sets + 1
  end <- start + (sets - 1)
  
  if (end > feature_count) {
    end <- feature_count
  }
  
  # print(paste(feature_count, sets, i, start, end))
  
  p <- caret::featurePlot(x=df_features[,start:end], y=df_features[,feature_count+1], plot="pairs", pch=20)
  print(p)
}
```

The scatter plots indicate some clear relationships between our target and predictor features, such as `PH` and `Oxygen Filter` or `PH` and `Alch Rel`. However, we also see clear correlations between some of the predictors, like `Carb Temp` and `Carb Pressure`. Overall, although our plots indicate some interesting relationships, they also underline the aforementioned possible issues with the data. For instance, many predictors have skewed distributions, and in some cases, missing data may be recorded as '0'.

### Feature-Target Correlations

We next quantify the relationships visualized above. In general, our model should focus on features showing stronger positive or negative correlations with `PH`. Features with correlations closer to zero will probably not provide any meaningful information on pH levels.

```{r echo=FALSE}
# Show feature correlations/target by decreasing correlation
stack(sort(cor(df_features[, feature_count + 1], df_features[,1:feature_count])[,], 
           decreasing=TRUE))
```

It appears that `Bowl Setpoint`, `Filler Level`, `Carb Flow`, `Pressure Vacuum`, and `Carb Rel` have the highest correlations (positive) with `PH`, while `Mnf Flow`, `Usage cont`, `Fill Pressure`, `Pressure Setpoint`, and `Hyd Pressure3` have the strongest negative correlations with `PH`. The other features have a weak or slightly negative correlation, which implies they have less predictive power.

### Multicollinearity

One problem that can occur with multiple regression is a correlation between predictive features, or multicollinearity. A quick check is to run correlations between all predictors.

```{r echo=FALSE, fig.height=8, fig.width=10}
# Calculate and plot the Multicollinearity
df_features <- df %>%
  dplyr::select(-c(`Brand Code`))
correlation = cor(df_features, use = 'pairwise.complete.obs')
corrplot(correlation, 'ellipse', type = 'lower', order = 'hclust',
         col=brewer.pal(n=8, name="RdYlBu"))
```

We can see that some variables are highly correlated with one another, such as `Balling Level` and `Carb Volume`, `Carb Rel`, `Alch Rel`, `Density`, and `Balling`, with a correlation between 0.75 and 1. When we start considering features for our models, we'll need to account for the correlations between features and avoid including pairs with strong correlations.

As a note, this dataset is challenging as many of the predictive features go hand-in-hand with other features and multicollinearity will be a problem.

### Near-Zero Variance

Lastly, we want to check for any features that show near zero-variance. Features that are the same across most of the instances will add little predictive information.

```{r}
nzv <- nearZeroVar(df, saveMetrics= TRUE)
nzv[nzv$nzv,][1:5,] %>% drop_na()
```

`Hyd Pressure1` displays near-zero variance. We will drop this feature prior to modeling.

## 2. Data Preparation

To summarize our data preparation and exploration, we distinguish our findings into a few categories below.

### Removed Fields

-   `MFR` has more than 8% missing values - remove this feature.
-   `Hyd Pressure1` shows little variance - remove this feature.

```{r}
# Remove the fields from our training data
df_clean <- df %>%
  dplyr::select(-c(MFR, `Hyd Pressure1`))
# remove the fields from our evaluation data
df_eval_clean <- df_eval %>%
  dplyr::select(-c(MFR, `Hyd Pressure1`))
  
```

### Missing Values

-   We had 4 rows with missing `PH` that need to be removed.
-   We replace missing values for `Brand Code` with "Unknown".
-   Impute remaining missing values using `kNN()` from the `VIM` package

```{r}
set.seed(100)
# drop rows with missing PH
df_clean <- df_clean %>%
  filter(!is.na(PH))

# Change Brand Code missing to 'Unknown' in our training data
brand_code <- df_clean %>%
  dplyr::select(`Brand Code`) %>%
  replace_na(list(`Brand Code` = 'Unknown'))
df_clean$`Brand Code` <- brand_code$`Brand Code`

# Change Brand Code missing to 'Unknown' in our evaluation data
brand_code <- df_eval_clean %>%
  dplyr::select(`Brand Code`) %>%
  replace_na(list(`Brand Code` = 'Unknown'))
df_eval_clean$`Brand Code` <- df_eval_clean$`Brand Code`

# There is an edge case where our Eval data might have a `Brand Code` not seen in our training set.
# If so, let's convert them to 'Unknown'.  This is appropriate since any model trained without the
# new value wouldn't be able to glean any info from it.
codes <- unique(df_clean$`Brand Code`)
df_eval_clean <- df_eval_clean %>%
  mutate(`Brand Code`  = if_else(`Brand Code` %in% codes, `Brand Code`, 'Unknown'))

# Use the kNN imputing method from VIM package to impute missing values in our training data
df_clean <- df_clean %>% 
  kNN(k=10) %>%
  dplyr::select(colnames(df_clean))

# Use the kNN imputing method from VIM package to impute missing values in our evaluation data
df_eval_clean <- df_eval_clean %>% 
  kNN(k=10) %>%
  dplyr::select(colnames(df_eval_clean))

```

### Outliers

We do not drop any outliers given all values seem reasonable.

### Convert Categorical to Dummy

`Brand Code` is a categorical variable with values A, B, C, D and Unknown. We convert it to a set of dummy columns for modeling.

```{r message=FALSE, warning=FALSE}

# Training data - Convert our `Brand Code` column into a set of dummy variables
df_clean_dummy <- dummyVars(PH ~ `Brand Code`, data = df_clean)
dummies <- predict(df_clean_dummy, df_clean)

# Get the dummy column names
dummy_cols <- sort(colnames(dummies))

# Make sure the new dummy columns are sorted in alpha order (to make sure our columns will match the eval dataset)
dummies <- as.tibble(dummies) %>%
  dplyr::select(dummy_cols)

# remove the original categorical feature
df_clean <- df_clean %>%
  dplyr::select(-`Brand Code`)

# add the new dummy columns to our main training dataframe
df_clean <- cbind(dummies, df_clean)

# Evaluation data - Convert our `Brand Code` column into a set of dummy variables
#df_eval_clean <- dummyVars(PH ~ `Brand Code`, data = df_eval_clean)
df_eval_clean$PH <- 1
eval_dummies <- predict(df_clean_dummy, df_eval_clean)

# Edge Case - if the eval dataset is doesn't have a specific `Brand Code`
# we will be missing the necessary dummy column.  Let's check and if necessary add 
# appropriate dummy columns with all 0's.
for (c in dummy_cols) {
  if (!(c %in% colnames(eval_dummies))) {
    eval_dummies[c] <- 0
  }
}

# Now sort the eval_dummy columns so they match the training set dummies
eval_dummy_cols <- sort(colnames(eval_dummies))
eval_dummies <- as.tibble(eval_dummies) %>%
  dplyr::select(eval_dummy_cols)

# remove the original categorical feature
df_eval_clean <- df_eval_clean %>%
  dplyr::select(-c(`Brand Code`, PH))

# add the new dummy columns to our main eval dataframe
df_eval_clean <- cbind(eval_dummies, df_eval_clean)

```

### Transform features with skewed distributions

Finally, as mentioned earlier in our data exploration, and our findings from our histogram plots, we can see that some of our features are highly skewed. To address this skewness, we scale, center, and apply the Box-Cox transformation to the skewed features using `preProcess` from `caret`. These transformations should result in distributions that better approximate normal and thus facilitate modeling.

```{r, echo=FALSE, fig.height=14, fig.width=8, message=FALSE, warning=FALSE}

# Drop the target, PH, we don't want to transform our target, only features
df_features <- df_clean %>%
  dplyr::select(-c(PH))

# Our evaluation (hold out data), note it didn't have the PH column
df_eval_features <- df_eval_clean

# Use caret pre-processing to handle scaling, norm'ing and BoxCox transforming our training data.
# We build the caret transformation on the training data, but will use that same xform against the 
# evaluation data.
preProcValues <- preProcess(
  df_features, 
  method = c("center", "scale", "BoxCox"))
df_transformed <- predict(preProcValues, df_features)
df_transformed$PH <- df_clean$PH
df_eval_transformed <- predict(preProcValues, df_eval_features)
preProcValues

```

Here are some plots to demonstrate the changes in distributions after the transformations:

```{r fig.height = 10, fig.width = 10}

# Prepare data for ggplot
gather_df <- df_transformed %>% 
  dplyr::select(-c(PH)) %>%
  gather(key = 'variable', value = 'value')

# Histogram plots of each variable
ggplot(gather_df) + 
  geom_histogram(aes(x=value, y = ..density..), bins=30) + 
  geom_density(aes(x=value), color='blue') +
  facet_wrap(. ~variable, scales='free', ncol=4)

```

As expected, the dummy variables, e.g. ``` ``Brand Code``A ```, appear binary. We still have bimodal features since we did not apply any feature engineering to address them. A few features, including `PSC Fill` and `Temperature`, still show skew, but they seem closer to normal. Our transformations are complete, and we can continue on to building our models.



## 3. Build Models


With a now solid understanding of our dataset, and with our data cleaned, we can now start to build candidate models. First, we split our cleaned dataset into training and testing sets (80% training, 20% testing). This split is necessary as the provided evaluation data set does not provide `PH` values, meaning we cannot measure our model performance against that dataset.

```{r}

training_set <- createDataPartition(df_transformed$PH, p=0.8, list=FALSE)
df_transformed1 <- df_transformed %>% dplyr::select (-PH)
X.train <- df_transformed1[training_set, ]
y.train <- df_transformed$PH[training_set]
X.test <- df_transformed1[-training_set, ]
y.test <- df_transformed$PH[-training_set]


dim(df_train)
dim(df_test)
```


#### Model 1 - Support Vector Machine (SVM)

SVM (Support Vector Machine) is a supervised machine learning algorithm which is mainly used to classify data into different classes.
Unlike most algorithms, SVM makes use of a hyperplane which acts like a decision boundary between the various classes.
SVM can be used to generate multiple separating hyperplanes such that the data is divided into segments and each segment contains only one kind of data.


```{r}

cl <- makePSOCKcluster(5)
registerDoParallel(cl)
set.seed(100)

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

svm_Linear <- train(x = X.train, y = y.train, method = "svmLinear",
                    trControl=trctrl,
                    preProcess = c("center", "scale"),
                    tuneLength = 10)

stopCluster(cl)

svm_Linear$results #summary(svm_Linear)

# Applying Model 1 against our Test Data:
svm_pred <- predict(svm_Linear, newdata = X.test)
test <- data.frame(cbind(svm_pred,y.test))
colnames(test) <- c("test","actual")
test <- test %>%
  mutate(pe = abs(actual - test)/actual)

MAPE <- (mean(test$pe))*100
MAPE


ggplot(test, aes(x = actual, y = test)) +
  geom_line() +
  geom_point()


# Bind results to a table to compare performance of our two models
results <- data.frame()
results <- data.frame(t(postResample(pred = svm_pred, obs = y.test))) %>% mutate(Model = "Support VEctor Machine (SVM)") %>% rbind(results)
results

```



#### Model 2 - Multivariate Adaptive Regression Splines

The approach used for the second model, Multivariate Adaptive Regression Splines (MARS), creates contrasting versions of each predictor to enter the model. These versions, features known as hinge functions, each represent an exclusive portion of the data. Such features are created iteratively for all model predictors, a process that is followed by "pruning" of individual features that do not contribute to the model.


```{r MARS, warning=FALSE}


options(max.print = 1e+06)

cl <- makePSOCKcluster(5)
registerDoParallel(cl)
set.seed(100)

mars_grid <- expand.grid(.degree = 1:2, .nprune = 2:15)
mars_model <- train(x = X.train, y = y.train, method = "earth", 
                    tuneGrid = mars.grid, 
                    preProcess = c("center", "scale"), 
                    tuneLength = 10)

stopCluster(cl)

summary(mars_model)

# Applying Model 2 against our Test Data:
mars_pred <- predict(mars_model, newdata = X.test)
test <- data.frame(cbind(mars_pred, y.test))
colnames(test) <- c("test","actual")
test <- test %>%
  mutate(pe = abs(actual - test)/actual)

MAPE <- (mean(test$pe))*100
MAPE


ggplot(test, aes(x = actual, y = test)) +
  geom_line() +
  geom_point()


# Bind results to a table to compare performance of our two models
results <- data.frame(t(postResample(pred = mars_pred, obs = y.test))) %>% mutate(Model = "Multivariate Adaptive Regression Splines (MARS)") %>% rbind(results)
results

```



### Model Summary

We evaluate our two models using three criteria: root mean squared error (RMSE), R-squared, and mean absolute error. The table below lists these criteria for each model.

```{r}

results %>% dplyr::select(Model, RMSE, Rsquared, MAE)

```

## 4. Model Selection

Based on evaluating both RMSE and $R^2$, MARS slightly outperformed SVM.


```{r}
varImp(mars_model)
``` 

## Predictions

We apply **Model #2 (MARS)** to the holdout evaluation set to predict the targets. We have saved these predictions as csv in the file `eval_predictions.csv`.


```{r, echo=F}
predictions <- predict(mars_model, df_eval_transformed)
df_eval$PH <- round(predictions, 2)
write.csv(df_eval, 'eval_predictions.csv', row.names=F)
```

